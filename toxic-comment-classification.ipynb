{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55acdad9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openlayer-ai/examples-gallery/blob/main/text-classification/sklearn/sentiment-analysis/sentiment-sklearn.ipynb)\n",
    "\n",
    "\n",
    "# <a id=\"top\">Sentiment analysis using sklearn</a>\n",
    "\n",
    "This notebook illustrates how sklearn models can be uploaded to the Openlayer platform.\n",
    "\n",
    "## <a id=\"toc\">Table of contents</a>\n",
    "\n",
    "1. [**Getting the data and training the model**](#1)\n",
    "    - [Downloading the dataset](#download)\n",
    "    - [Training the model](#train)\n",
    "    \n",
    "\n",
    "2. [**Using Openlayer's Python API**](#2)\n",
    "    - [Instantiating the client](#client)\n",
    "    - [Creating a project](#project)\n",
    "    - [Uploading datasets](#dataset)\n",
    "    - [Uploading models](#model)\n",
    "        - [Shell models](#shell)\n",
    "        - [Full models](#full-model)\n",
    "    - [Committing and pushing to the platform](#commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1a76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -e \"requirements.txt\" ]; then\n",
    "    curl \"https://raw.githubusercontent.com/openlayer-ai/examples-gallery/main/text-classification/sklearn/sentiment-analysis/requirements.txt\" --output \"requirements.txt\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813990ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.20 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: pandas==1.1.4 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.1.4)\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages/python_dateutil-2.8.2-py3.9.egg (from pandas==1.1.4->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from pandas==1.1.4->-r requirements.txt (line 2)) (2022.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 3)) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from scikit-learn==0.24.2->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0e018",
   "metadata": {},
   "source": [
    "## <a id=\"1\"> 1. Getting the data and training the model </a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "In this first part, we will get the dataset, pre-process it, split it into training and validation sets, and train a model. Feel free to skim through this section if you are already comfortable with how these steps look for an sklearn model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "atlantic-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f656146",
   "metadata": {},
   "source": [
    "### <a id=\"download\">Downloading the dataset </a>\n",
    "\n",
    "\n",
    "We have stored the dataset on the following S3 bucket. If, for some reason, you get an error reading the csv directly from it, feel free to copy and paste the URL in your browser and download the csv files. Alternatively, you can also find the original datasets on [this Kaggle competition](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset?select=testdata.manual.2009.06.14.csv). The training set in this example corresponds to the first 20,000 rows of the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20616bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTest dataset has no labels, so we will have to split the training set into a test partition\\n\\ndf_test = pd.read_csv(\\n    \"./quora-insincere-questions-classification/test.csv\",\\n    encoding=\\'ISO-8859-1\\'\\n)\\n\\n# Load the labels\\ndf_test_labels = pd.read_csv(\\'test_labels.csv\\')\\n\\n# Join the two dataframes by the shared column\\nmerged_df = pd.merge(df_test, df_test_labels, on=\\'id\\')\\n\\n# Save the merged dataframe to a new CSV file\\nmerged_df.to_csv(\\'test_with_labels.csv\\', index=False)\\n\\n# Load the labels\\ndf_test_labels = pd.read_csv(\\'test_labels.csv\\')\\n\\n# Join the two dataframes by the shared column\\nmerged_df = pd.merge(df_test, df_test_labels, on=\\'id\\')\\n\\n# Save the merged dataframe to a new CSV file\\nmerged_df.to_csv(\\'test_with_labels.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Test dataset has no labels, so we will have to split the training set into a test partition\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    \"./quora-insincere-questions-classification/test.csv\",\n",
    "    encoding='ISO-8859-1'\n",
    ")\n",
    "\n",
    "# Load the labels\n",
    "df_test_labels = pd.read_csv('test_labels.csv')\n",
    "\n",
    "# Join the two dataframes by the shared column\n",
    "merged_df = pd.merge(df_test, df_test_labels, on='id')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('test_with_labels.csv', index=False)\n",
    "\n",
    "# Load the labels\n",
    "df_test_labels = pd.read_csv('test_labels.csv')\n",
    "\n",
    "# Join the two dataframes by the shared column\n",
    "merged_df = pd.merge(df_test, df_test_labels, on='id')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('test_with_labels.csv', index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incomplete-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./quora-insincere-questions-classification/train.csv\",\n",
    "    encoding='ISO-8859-1', \n",
    ")\n",
    "\n",
    "df.drop(df[df['question_text'].str.len() > 1000].index, inplace = True)\n",
    "\n",
    "def split_dataframe(df):\n",
    "    train, test = train_test_split(df, test_size=0.25)\n",
    "    return train, test\n",
    "\n",
    "df_train, df_val = split_dataframe(df)\n",
    "df_test = pd.read_csv(\n",
    "    \"./quora-insincere-questions-classification/test.csv\",\n",
    "    encoding='ISO-8859-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5d9727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['qid', 'question_text', 'target'], dtype='object')\n",
      "Index(['qid', 'question_text', 'target'], dtype='object')\n",
      "Index(['qid', 'question_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_val.columns)\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e435aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>510043</th>\n",
       "      <td>63e146d4099adc4570d3</td>\n",
       "      <td>Alex Jones is smarter than you. How does that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76788</th>\n",
       "      <td>0f07608c8ac54878338c</td>\n",
       "      <td>If the AI bots at Facebook did not find an inc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510870</th>\n",
       "      <td>640aca4890b1eb31ea1c</td>\n",
       "      <td>Why was steel from the skyscraper were taken a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266115</th>\n",
       "      <td>3415edce8dab51c88548</td>\n",
       "      <td>It seems that many countries have made private...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667538</th>\n",
       "      <td>82b9e027fbb94bfbf619</td>\n",
       "      <td>What was the ugliest car ever made?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "510043  63e146d4099adc4570d3   \n",
       "76788   0f07608c8ac54878338c   \n",
       "510870  640aca4890b1eb31ea1c   \n",
       "266115  3415edce8dab51c88548   \n",
       "667538  82b9e027fbb94bfbf619   \n",
       "\n",
       "                                            question_text  target  \n",
       "510043  Alex Jones is smarter than you. How does that ...       1  \n",
       "76788   If the AI bots at Facebook did not find an inc...       0  \n",
       "510870  Why was steel from the skyscraper were taken a...       0  \n",
       "266115  It seems that many countries have made private...       0  \n",
       "667538                What was the ugliest car ever made?       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012a4f1",
   "metadata": {},
   "source": [
    "### <a id=\"train\">Training the model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "multiple-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthibsamadder/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vect',\n",
       "                 CountVectorizer(min_df=100, ngram_range=(1, 2),\n",
       "                                 stop_words='english')),\n",
       "                ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model = Pipeline([(\"count_vect\", \n",
    "                           CountVectorizer(min_df=100, \n",
    "                                           ngram_range=(1, 2), \n",
    "                                           stop_words=\"english\"),),\n",
    "                          (\"lr\", LogisticRegression()),])\n",
    "sklearn_model.fit(df_train.question_text, df_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae4d857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    306162\n",
      "           1       0.67      0.38      0.49     20369\n",
      "\n",
      "    accuracy                           0.95    326531\n",
      "   macro avg       0.82      0.68      0.73    326531\n",
      "weighted avg       0.94      0.95      0.94    326531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_val, y_val = df_val.question_text, df_val.target\n",
    "print(classification_report(y_val, sklearn_model.predict(x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9193bec1",
   "metadata": {},
   "source": [
    "## <a id=\"2\"> 2. Using Openlayer's Python API</a>\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "Now it's time to upload the datasets and model to the Openlayer platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8440a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openlayer\n",
      "Version: 0.0.0a16\n",
      "Summary: The official Python API library for Openlayer: the Testing and Debugging Platform for AI\n",
      "Home-page: https://github.com/openlayer-ai/openlayer-python\n",
      "Author: Unbox Inc.\n",
      "Author-email: \n",
      "License: \n",
      "Location: /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages\n",
      "Requires: jupyter, marshmallow, pandas, requests, requests, requests-toolbelt, tqdm, urllib3\n",
      "Required-by: \n",
      "Requirement already satisfied: openlayer==0.0.0a16 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (0.0.0a16)\n",
      "Requirement already satisfied: jupyter in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (1.0.0)\n",
      "Requirement already satisfied: pandas in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (1.1.4)\n",
      "Requirement already satisfied: requests in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (4.62.3)\n",
      "Requirement already satisfied: marshmallow in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (3.19.0)\n",
      "Requirement already satisfied: requests-toolbelt in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (0.9.1)\n",
      "Requirement already satisfied: urllib3>=1.26.14 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from openlayer==0.0.0a16) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from requests->openlayer==0.0.0a16) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from requests->openlayer==0.0.0a16) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from requests->openlayer==0.0.0a16) (2021.10.8)\n",
      "Requirement already satisfied: notebook in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (6.4.12)\n",
      "Requirement already satisfied: qtconsole in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (5.3.1)\n",
      "Requirement already satisfied: jupyter-console in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (6.4.3)\n",
      "Requirement already satisfied: nbconvert in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (6.5.0)\n",
      "Requirement already satisfied: ipykernel in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (6.14.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter->openlayer==0.0.0a16) (7.7.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from marshmallow->openlayer==0.0.0a16) (23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages/python_dateutil-2.8.2-py3.9.egg (from pandas->openlayer==0.0.0a16) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from pandas->openlayer==0.0.0a16) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from pandas->openlayer==0.0.0a16) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->openlayer==0.0.0a16) (1.16.0)\n",
      "Requirement already satisfied: appnope in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (1.6.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (8.4.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (7.3.4)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (1.5.5)\n",
      "Requirement already satisfied: psutil in /Users/parthibsamadder/.local/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (5.8.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (6.1)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipykernel->jupyter->openlayer==0.0.0a16) (5.2.2.post1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->openlayer==0.0.0a16) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->openlayer==0.0.0a16) (5.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->openlayer==0.0.0a16) (3.6.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipywidgets->jupyter->openlayer==0.0.0a16) (1.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter-console->jupyter->openlayer==0.0.0a16) (3.0.29)\n",
      "Requirement already satisfied: pygments in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jupyter-console->jupyter->openlayer==0.0.0a16) (2.12.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (4.11.1)\n",
      "Requirement already satisfied: bleach in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (5.0.0)\n",
      "Requirement already satisfied: defusedxml in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (0.3)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (3.0.1)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (4.10.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (0.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (2.1.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (0.6.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbconvert->jupyter->openlayer==0.0.0a16) (1.1.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from notebook->jupyter->openlayer==0.0.0a16) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from notebook->jupyter->openlayer==0.0.0a16) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from notebook->jupyter->openlayer==0.0.0a16) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from notebook->jupyter->openlayer==0.0.0a16) (0.15.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from notebook->jupyter->openlayer==0.0.0a16) (0.14.1)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from qtconsole->jupyter->openlayer==0.0.0a16) (2.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backcall in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (65.5.1)\n",
      "Requirement already satisfied: stack-data in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (4.8.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->openlayer==0.0.0a16) (4.6.0)\n",
      "Requirement already satisfied: fastjsonschema in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->openlayer==0.0.0a16) (2.15.3)\n",
      "Requirement already satisfied: wcwidth in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->openlayer==0.0.0a16) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from terminado>=0.8.3->notebook->jupyter->openlayer==0.0.0a16) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter->openlayer==0.0.0a16) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter->openlayer==0.0.0a16) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from bleach->nbconvert->jupyter->openlayer==0.0.0a16) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages/attrs-21.4.0-py3.9.egg (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->openlayer==0.0.0a16) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter->openlayer==0.0.0a16) (0.18.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->openlayer==0.0.0a16) (1.15.0)\n",
      "Requirement already satisfied: executing in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->openlayer==0.0.0a16) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /Users/parthibsamadder/miniconda3/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->openlayer==0.0.0a16) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip show openlayer\n",
    "!pip install openlayer==0.0.0a16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9049c05",
   "metadata": {},
   "source": [
    "### <a id=\"client\">Instantiating the client</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "medium-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openlayer\n",
    "\n",
    "openlayer.api.OPENLAYER_ENDPOINT = \"https://api-staging.openlayer.com/v1\"\n",
    "client = openlayer.OpenlayerClient(\"Vnua5sn7Z9bVtrrhEhSeJnzLPNtXTDLx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae672f2",
   "metadata": {},
   "source": [
    "### <a id=\"project\">Creating a project on the platform</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750132b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found your project. Navigate to https://staging.openlayer.com/openlayer/8f02885f-bc0c-4f79-bc20-23f59b56d470 to see it.\n"
     ]
    }
   ],
   "source": [
    "from openlayer import TaskType\n",
    "\n",
    "project = client.create_or_load_project(\n",
    "    name=\"Parthib and Vikas: Insincere Questions\",\n",
    "    task_type=TaskType.TextClassification,\n",
    "    description=\"Classifying quora questions by sincerity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb6823",
   "metadata": {},
   "source": [
    "### <a id=\"dataset\">Uploading datasets</a>\n",
    "\n",
    "Before adding the datasets to a project, we need to do two things:\n",
    "1. Enhance the dataset with additional columns to make it comprehensive, such as adding a column for labels and one for model predictions (if you're uploading a model as well).\n",
    "2. Prepare a `dataset_config.yaml` file. This is a file that contains all the information needed by the Openlayer platform to utilize the dataset. It should include the column names, the class names, etc. For details on the fields of the `dataset_config.yaml` file, see the [API reference](https://reference.openlayer.com/reference/api/openlayer.OpenlayerClient.add_dataset.html#openlayer.OpenlayerClient.add_dataset).\n",
    "\n",
    "Let's start by enhancing the datasets with the extra columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84023241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/619r41qd3sgb8f0g1cf930t80000gn/T/ipykernel_9812/240082985.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train[\"predictions\"] = sklearn_model.predict_proba(df_train['question_text']).tolist()\n",
      "/var/folders/vl/619r41qd3sgb8f0g1cf930t80000gn/T/ipykernel_9812/240082985.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val[\"predictions\"] = sklearn_model.predict_proba(df_val['question_text']).tolist()\n"
     ]
    }
   ],
   "source": [
    "# Adding the column with the predictions (since we'll also upload a model later)\n",
    "df_train[\"predictions\"] = sklearn_model.predict_proba(df_train['question_text']).tolist()\n",
    "df_val[\"predictions\"] = sklearn_model.predict_proba(df_val['question_text']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3bab4",
   "metadata": {},
   "source": [
    "Now, we can prepare the `dataset_config.yaml` files for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3dcc96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables that will go into the `dataset_config.yaml` file\n",
    "column_names = list(df_train.columns)\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "label_column_name = \"target\"\n",
    "prediction_scores_column_name = \"predictions\"\n",
    "text_column_name = \"question_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904c0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "# Note the camelCase for the dict's keys\n",
    "training_dataset_config = {\n",
    "    \"classNames\": class_names,\n",
    "    \"columnNames\": column_names,\n",
    "    \"textColumnName\": text_column_name,\n",
    "    \"label\": \"training\",\n",
    "    \"labelColumnName\": label_column_name,\n",
    "    \"predictionScoresColumnName\": prediction_scores_column_name,\n",
    "}\n",
    "\n",
    "with open(\"training_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(training_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b4284dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "validation_dataset_config = copy.deepcopy(training_dataset_config)\n",
    "\n",
    "# In our case, the only field that changes is the `label`, from \"training\" -> \"validation\"\n",
    "validation_dataset_config[\"label\"] = \"validation\"\n",
    "\n",
    "with open(\"validation_dataset_config.yaml\", \"w\") as dataset_config_file:\n",
    "    yaml.dump(validation_dataset_config, dataset_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f0a9761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['qid', 'question_text', 'target', 'predictions'], dtype='object')\n",
      "Staged the `training` resource!\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n",
    "# Training set\n",
    "project.add_dataframe(\n",
    "    dataset_df=df_train,\n",
    "    dataset_config_file_path=\"training_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbf393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `validation` resource!\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "project.add_dataframe(\n",
    "    dataset_df=df_val,\n",
    "    dataset_config_file_path=\"validation_dataset_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d63bce",
   "metadata": {},
   "source": [
    "We can check that both datasets are now staged using the `project.status()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d22d1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are staged, waiting to be committed:\n",
      "\t - training\n",
      "\t - validation\n",
      "Use the `commit` method to add a commit message to your changes.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e1834",
   "metadata": {},
   "source": [
    "### <a id=\"model\">Uploading models</a>\n",
    "\n",
    "When it comes to uploading models to the Openlayer platform, there are two options:\n",
    "\n",
    "- The first one is to upload a **shell model**. Shell models are the most straightforward way to get started. They are comprised of metadata and all of the analysis are done via its predictions (which are [uploaded with the datasets](#dataset)).\n",
    "- The second one is to upload a **full model**, with artifacts. When a full model is uploaded, it becomes available in the platform and it becomes possible to perform what-if analysis, use all the explainability techniques available, and perform a series of robustness assessments with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7e082",
   "metadata": {},
   "source": [
    "#### <a id=\"shell\">Shell models</a>\n",
    "\n",
    "To upload a shell model, we only need to define its name, the architecture type, and add some metadata that will be rendered in the platform to help us identify it. This information should be saved to a `model_config.yaml` file.\n",
    "\n",
    "Let's create a `model_config.yaml` file for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "865fb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Sentiment analysis model\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"metadata\": {  # Can add anything here, as long as it is a dict\n",
    "        \"model_type\": \"Logistic Regression\",\n",
    "        \"regularization\": \"None\",\n",
    "    },\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open(\"model_config.yaml\", \"w\") as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3613129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `model` resource!\n"
     ]
    }
   ],
   "source": [
    "project.add_model(\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e2bb1",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762619fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are staged, waiting to be committed:\n",
      "\t - training\n",
      "\t - model\n",
      "\t - validation\n",
      "Use the `commit` method to add a commit message to your changes.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec5f35",
   "metadata": {},
   "source": [
    "Since in this example, we're interested in uploading a full model, let's unstage the shell model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1796f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed resource `model` from the staging area.\n"
     ]
    }
   ],
   "source": [
    "project.restore(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39ff1e",
   "metadata": {},
   "source": [
    "#### <a id=\"full-model\"> Full models </a>\n",
    "\n",
    "To upload a full model to Openlayer, you will need to create a model package, which is nothing more than a folder with all the necessary information to run inference with the model. The package should include the following:\n",
    "1. A `requirements.txt` file listing the dependencies for the model.\n",
    "2. Serialized model files, such as model weights, encoders, etc., in a format specific to the framework used for training (e.g. `.pkl` for sklearn, `.pb` for TensorFlow, and so on.)\n",
    "3. A `prediction_interface.py` file that acts as a wrapper for the model and implements the `predict_proba` function. \n",
    "\n",
    "Other than the model package, a `model_config.yaml` file is needed, with information about the model to the Openlayer platform, such as the framework used, feature names, and categorical feature names.\n",
    "\n",
    "Lets prepare the model package one piece at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e501c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: model_package: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Creating the model package folder (we'll call it `model_package`)\n",
    "!mkdir model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f65e2e",
   "metadata": {},
   "source": [
    "**1. Adding the `requirements.txt` to the model package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "772887d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp requirements.txt model_package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7a767",
   "metadata": {},
   "source": [
    "**2. Serializing the model and other objects needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02c65dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# Trained model pipeline\n",
    "with open('model_package/model.pkl', 'wb') as handle:\n",
    "    pickle.dump(sklearn_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7d1a1",
   "metadata": {},
   "source": [
    "**3. Writing the `prediction_interface.py` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51ae9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_package/prediction_interface.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_package/prediction_interface.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "\n",
    "class SklearnModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"This is where the serialized objects needed should\n",
    "        be loaded as class attributes.\"\"\"\n",
    "\n",
    "        with open(PACKAGE_PATH / \"model.pkl\", \"rb\") as model_file:\n",
    "            self.model = pickle.load(model_file)\n",
    "\n",
    "    def predict_proba(self, input_data_df: pd.DataFrame):\n",
    "        \"\"\"Makes predictions with the model. Returns the class probabilities.\"\"\"\n",
    "        text_column = input_data_df.columns[0]\n",
    "        return self.model.predict_proba(input_data_df[text_column])\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Function that returns the wrapped model object.\"\"\"\n",
    "    return SklearnModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54b757",
   "metadata": {},
   "source": [
    "**Creating the `model_config.yaml`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67bb695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "model_config = {\n",
    "    \"name\": \"Sentiment analysis model\",\n",
    "    \"architectureType\": \"sklearn\",\n",
    "    \"classNames\": class_names,\n",
    "}\n",
    "\n",
    "with open('model_config.yaml', 'w') as model_config_file:\n",
    "    yaml.dump(model_config, model_config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605e31b",
   "metadata": {},
   "source": [
    "Lets check that the model package contains everything needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "maritime-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openlayer.validators import model_validators\n",
    "\n",
    "model_validator = model_validators.ModelValidator(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data = df_val[[\"question_text\"]].iloc[:10, :]\n",
    ")\n",
    "model_validator.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a7554",
   "metadata": {},
   "source": [
    "Now, we are ready to add the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0341d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged the `model` resource!\n"
     ]
    }
   ],
   "source": [
    "project.add_model(\n",
    "    model_package_dir=\"model_package\",\n",
    "    model_config_file_path=\"model_config.yaml\",\n",
    "    sample_data=df_val[[\"question_text\"]].iloc[:10, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756c33f",
   "metadata": {},
   "source": [
    "We can check that both datasets and model are staged using the `project.status()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cddbb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are staged, waiting to be committed:\n",
      "\t - training\n",
      "\t - model\n",
      "\t - validation\n",
      "Use the `commit` method to add a commit message to your changes.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc2577",
   "metadata": {},
   "source": [
    "### <a id=\"commit\"> Committing and pushing to the platform </a>\n",
    "\n",
    "Finally, we can commit the first project version to the platform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cea48e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Committed!\n"
     ]
    }
   ],
   "source": [
    "project.commit(\"Contains the full model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ac9642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following resources are committed, waiting to be pushed:\n",
      "\t - training\n",
      "\t - model\n",
      "\t - validation\n",
      "Commit message from Thu May 11 13:51:30 2023:\n",
      "\t Contains the full model\n",
      "Use the `push` method to push your changes to the platform.\n"
     ]
    }
   ],
   "source": [
    "project.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c3e6527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing changes to the platform with the commit message: \n",
      "\t - Message: Contains the full model \n",
      "\t - Date: Thu May 11 13:51:30 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[34m████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\u001b[0m| 106M/106M [00:09<00:00, 11.7MB/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed!\n"
     ]
    }
   ],
   "source": [
    "project.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b35d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e4354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
